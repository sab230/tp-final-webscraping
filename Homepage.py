import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from function import *

st.set_page_config(
    page_title="Le petit blog ",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# This is a header. This is an *extremely* cool app!"
    }
)

with st.sidebar:
    st.write("Sabrina PAUVADAY")
    st.write("J'ai choisi BeautifulSoup (bs4) pour son interface conviviale, sa capacit√© √† naviguer dans le DOM, sa robustesse face aux erreurs, sa compatibilit√© avec Python, et son int√©gration ais√©e avec d'autres biblioth√®ques. Ces caract√©ristiques en font un choix populaire pour le web scraping dans l'application, comme expliqu√© dans la description avec le lien vers le site scrapp√©.")
    st.write("https://www.blogdumoderateur.com/")

    # Ajouter un bouton "Historique" dans la barre lat√©rale
    if st.button("Historique"):
        # Charger les donn√©es de la base de donn√©es pour l'historique
        database = DataBase('journaldugeek')  # Assurez-vous que la connexion √† la base de donn√©es est r√©tablie
        queries_df = pd.read_sql_query("SELECT * FROM articles;", database.connection)

        # Afficher la page historique
        st.title("Historique des Requ√™tes")

        if not queries_df.empty:
            st.dataframe(queries_df)
            st.markdown("## T√©l√©charger l'historique des requ√™tes au format CSV")
            st.download_button("T√©l√©charger l'historique des requ√™tes", queries_df.to_csv(index=False), "historique_requetes.csv", "text/csv")
        else:
            st.info("Aucune requ√™te dans l'historique.")

st.title("Le petit blog ")
st.write("Bonjour et bienvenu sur le blog qui permet d'√™tre inform√© sur les derni√®res actualit√©s!")

def scrape_page(search, page_number):
    json_data = []
    
    database = DataBase('journaldugeek')
    # Nombre d'articles par page (ajustez en fonction de la pagination r√©elle du site)
    articles_per_page = 5
    start_index = (page_number - 1) * articles_per_page

    url = f"https://www.blogdumoderateur.com/page/{page_number}/?s={search}"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    
    articles = soup.find_all('article')

    for article in articles[start_index:start_index + articles_per_page]:
        title = article.find('h3').text.replace('\xa0', '')
        image = article.find('img')['src']
        link = article.find('h3').parent['href']
        theme = article.find('span', 'favtag').text
        date = article.find('time').get('datetime').split('T')[0]

        json_data.append({'title': title, 'image': image, 'link': link, 'theme': theme, 'date': date})
        try:
            database.add_row('articles', titre=title, 
            image=image, 
            link=link, theme=theme, author=date)
        except:
            pass

    return json_data

submitted = False

with st.form('Form 1'):
    name = st.text_input('Tapez votre recherche')

    # Demander √† l'utilisateur le nombre de pages √† afficher
    num_pages = st.slider("Nombre de pages √† afficher", min_value=1, max_value=10, value=3)

    if st.form_submit_button('Rechercher'):
        # Afficher le nombre d'articles sp√©cifi√© sur le nombre de pages sp√©cifi√©
        scrape_data = []
        for page_number in range(1, num_pages + 1):
            scrape_data.extend(scrape_page(name, page_number))

        df = pd.DataFrame(scrape_data)
        st.write(df)

        # Afficher les images avec une taille sp√©cifi√©e
        for index, row in df.iterrows():
            st.image(row['image'], caption=row['title'], use_column_width=True, width=200)

        # Enregistrer dans un fichier CSV avec le nom de la requ√™te
        query_filename = f"{name}.csv"
        df.to_csv(query_filename, index=False)

        submitted = True

if submitted:
    st.sidebar.download_button('T√©l√©charger le DataFrame', df.to_csv(), 'data.csv', 'text/csv')
